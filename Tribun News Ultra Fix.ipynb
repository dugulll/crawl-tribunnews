{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "def cleanhtml(raw_html): #membersihkan paragraf dari tag html\n",
    "  cleanr = re.compile('<.*?>') #set tag yang akan di hapus\n",
    "  cleantext = re.sub(cleanr, '', raw_html) #hapus tag htmlnya\n",
    "  return cleantext\n",
    "\n",
    "def crawl_1para(soup, raw_para): #menarik paragraf dalam 1 halaman berita\n",
    "    for para in soup.find_all('p'): #mencari paragraf dengan menggunakan tag html p\n",
    "        flag1 = \"\" #inisiasi flag 1\n",
    "        flag2 = \"\" #insiasi flag 2\n",
    "        flag1 = str((para.get('class'))) #mendapatkan class dari teks paragraf\n",
    "        try:\n",
    "            flag2 = (para.find('a').get('title')) #mendapatkan title dari teks paragraf\n",
    "        except:\n",
    "            flag2 = \"\" #jika tidak ada di abaikan dengan menggunakan string kosong\n",
    "            pass\n",
    "        flag3 = str(para).lower().find('selanjutnya') #mencari kata selanjutnya\n",
    "        flag4 = str(para).lower().find('selengkapnya') #mencari kata selengkapnya\n",
    "        flag5 = str(para).lower().find('berikutnya') #mencari kata berikutnya\n",
    "        \n",
    "        if(flag1==\"['baca']\" or flag2==\"halflink\" or flag3!=-1 or flag4!=-1 or flag5!=-1):\n",
    "            pass #jika ditemukan kata tersebut berita yang diambil adalah iklan\n",
    "        else: #selain itu paragraf akan di simpan di dalam raw_para\n",
    "            para = para.text \n",
    "            raw_para = raw_para + para\n",
    "    return raw_para\n",
    "\n",
    "def crawl_headlines(soup): #menarik headlines semua berita di dalam 1 halaman index\n",
    "    headlines = soup.find_all('a', href=True, title=True, class_=\"fbo f18 ln24\") #mencari headlines\n",
    "    return headlines\n",
    "\n",
    "def crawl_times(soup, times): #menarik data waktu\n",
    "    try:\n",
    "        times.append(soup.find('time').text) #memasukan data waktu berita ke dalam list\n",
    "    except:\n",
    "        times.append('') #jika data tidak ditemukan memasukan string kosong ke dalam list\n",
    "    return times\n",
    "\n",
    "def crawl_lokasi(soup, locations): #menarik data lokasi berita\n",
    "    try: \n",
    "        locations.append(soup.find('strong').text) #memasukan data lokasi berita ke dalam list\n",
    "    except:\n",
    "        locations.append('') #jika data tidak ditemukan memasukan string kosong ke dalam list\n",
    "    return locations\n",
    "\n",
    "def crawl_keyword(soup, keywords): #menarik data tag berita\n",
    "    keywords.append(soup.find(attrs={'name':\"news_keywords\"}).get('content')) #memasukan tag berita ke dalam list\n",
    "    return keywords\n",
    "\n",
    "def crawl_news_para(soup, paragraphs): #mengambil data paragraf 1 berita\n",
    "    raw_para = \"\" #inisiasi data paragraf\n",
    "    raw_para = crawl_1para(soup, raw_para) #mengambil data paragraf di dalam 1 halaman\n",
    "    while True: #pencarian nama paragraf dilakukan hingga halaman terakhir paragraf\n",
    "        try:\n",
    "            url = soup.find('a', class_=(\"bgblue white ptb5 plr10 f18\")).get('href') #mencari nama tombol url\n",
    "            txt = soup.find('a', class_=(\"bgblue white ptb5 plr10 f18\")).text #mencari nama tombolnya\n",
    "            if(txt == 'Halaman selanjutnya '): #jika masih ada halaman selanjutnya\n",
    "                try:\n",
    "                    sauce = urllib.request.urlopen(url) #mengakses url\n",
    "                    soup = bs.BeautifulSoup(sauce,'lxml') #membaca html\n",
    "                    raw_para = crawl_1para(soup, raw_para) #Mengambil data paragraf di dalam 1 halaman\n",
    "                except(HTTPError, ValueError): #Terjadi koneksi error mengulangi akses url\n",
    "                    pass\n",
    "            else: #Halaman terakhir berita\n",
    "                break\n",
    "        except: #Berita hanya 1 halaman, pencarian paragraf dihentikan\n",
    "            break\n",
    "    clean_para = cleanhtml(raw_para) #membersihkan data paragraf\n",
    "    paragraphs.append(clean_para) #memasukan paragraf yang sudah bersih ke dalam list\n",
    "    return paragraphs\n",
    "\n",
    "def crawl_editor(soup, editors): #menarik data editor\n",
    "    try:\n",
    "        raw_editor = soup.find('div', attrs={'id':'editor'}).string #mencari editor di dalam url\n",
    "        editors.append(raw_editor[11:-2]) #memasukan editor ke dalam list\n",
    "    except: #jika ditemukan editor dimasukan string kosong\n",
    "        editors.append('')\n",
    "    return editors\n",
    "\n",
    "def outside_crawl(tags, labels, date_start, date_end):\n",
    "    #Inisiasi\n",
    "    titles = [] #judul berita/headlines\n",
    "    links = [] #link berita\n",
    "    dates = [] #tanggal berita\n",
    "    resumes = [] #resume berita\n",
    "    label = [] #label berita\n",
    "    tag = [] #tag yang digunakan dalam pencarian\n",
    "    \n",
    "    for i in tqdm_notebook(range(len(tags)), desc='Outside Crawler Progress'): #Loading bar, Perulangan sebanyak jumlah tag\n",
    "        url = 'http://www.tribunnews.com/tag/' + tags[i] #set url tag\n",
    "        sauce = urllib.request.urlopen(url) #mengakses url\n",
    "        soup = bs.BeautifulSoup(sauce,'lxml') #membaca html url\n",
    "\n",
    "        while True: #selama bukan halaman akhir dan tanggal masih dalam pencarian\n",
    "            try:\n",
    "                url = soup.find(\"a\", text=(\"Next\")).get('href') #mencari link yang mengarahkan ke halaman berikutnya\n",
    "                try:\n",
    "                    sauce = urllib.request.urlopen(url) #mengakses halaman berikutnya\n",
    "                    soup = bs.BeautifulSoup(sauce,'lxml') #membaca halaman berikutnya\n",
    "                except (HTTPError, ValueError): #jika terjadi koneksi error\n",
    "                    print('error!') #print error dan mengulang try\n",
    "                    pass\n",
    "\n",
    "                headlines = crawl_headlines(soup) #crawl semua headlines dalam 1 halaman index\n",
    "                for x in headlines: #menunjuk headlines 1 persatu\n",
    "                    link = x.get('href') #mencari link berita di dalam headlines\n",
    "                    \n",
    "                    awal = link.find('/', 28) #mencari tanggal berita di dalam url\n",
    "                    date_now = link[awal+1:awal+11] #mencari tanggal berita di dalam url\n",
    "\n",
    "                    if(date_now > date_end): #Cek jika tanggal berita sekarang lebih baru dibandingkan tanggal akhir\n",
    "                        continue #Berita tidak akan dimasukan ke dalam list\n",
    "\n",
    "                    if(date_now < date_start): #Cek jika tanggal berita sekarang lebih lama dibandingkan tanggal awal\n",
    "                        break #Pencarian dihentikan\n",
    "                    \n",
    "                    titles.append(x.get('title')) #memasukan judul tiap berita ke dalam list\n",
    "                    links.append(link) #memasukan link tiap berita ke dalam list\n",
    "                    label.append(labels[i]) #memasukan label tiap berita ke dalam list\n",
    "                    tag.append(tags[i]) #memasukan tag yang digunakan dalam pencarian ke dalam list\n",
    "\n",
    "            #Jika halaman terakhir\n",
    "            except AttributeError:\n",
    "                break\n",
    "    return titles, links, label, tag\n",
    "\n",
    "def inside_crawl(links):\n",
    "    #Insiasi\n",
    "    times = [] #waktu\n",
    "    locations = [] #lokasi\n",
    "    keywords = [] #3-4 tags berita\n",
    "    paragraphs = [] #paragraf/isi berita\n",
    "    editors = [] #editor\n",
    "    \n",
    "    for i in tqdm_notebook(range(len(links)), desc='Inside Crawl Progress'): #loading bar, menarik data dari setiap link\n",
    "        url = links[i] #menunjuk link 1 persatu\n",
    "        sauce = urllib.request.urlopen(url) #mengakses html link\n",
    "        soup = bs.BeautifulSoup(sauce,'lxml') #membaca html link\n",
    "        \n",
    "        #Crawling\n",
    "        times = crawl_times(soup, times) #waktu\n",
    "        locations = crawl_lokasi(soup, locations) #lokasi\n",
    "        keywords = crawl_keyword(soup, keywords) #3-4 tags berita\n",
    "        paragraphs = crawl_news_para(soup, paragraphs) #paragraf/isi  berita\n",
    "        editors = crawl_editor(soup, editors) #editor\n",
    "\n",
    "    return times, locations, keywords, paragraphs, editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991455e79fc34fe99c3f8a3926f64cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Outside Crawler Progress', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ad2580bc5f450c842fba175da64a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Inside Crawl Progress', max=2543), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>dates</th>\n",
       "      <th>location</th>\n",
       "      <th>link</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>label</th>\n",
       "      <th>tag</th>\n",
       "      <th>keywords</th>\n",
       "      <th>editor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silaturahmi dengan Pimpinan Ponpes se-Malang R...</td>\n",
       "      <td>Senin, 29 Oktober 2018 21:37 WIB</td>\n",
       "      <td>TRIBUNNEWS.COM, MALANG -</td>\n",
       "      <td>http://www.tribunnews.com/regional/2018/10/29/...</td>\n",
       "      <td>TRIBUNNEWS.COM, MALANG - Calon wakil presiden ...</td>\n",
       "      <td>jokowi</td>\n",
       "      <td>joko-widodo</td>\n",
       "      <td>Pilpres 2019, Maruf Amin, Malang, cawapres, Jo...</td>\n",
       "      <td>Sugiyarto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indonesia Memiliki Program Pengurangan Polusi ...</td>\n",
       "      <td>Senin, 29 Oktober 2018 20:21 WIB</td>\n",
       "      <td>TRIBUNNEWS.COM, BADUNG</td>\n",
       "      <td>http://www.tribunnews.com/nasional/2018/10/29/...</td>\n",
       "      <td>TRIBUNNEWS.COM, BADUNG - Presiden Joko Widodo ...</td>\n",
       "      <td>jokowi</td>\n",
       "      <td>joko-widodo</td>\n",
       "      <td>Joko Widodo, Our Ocean Conference 2018, Nusa D...</td>\n",
       "      <td>Toni Bramantoro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pemerintah Belum Pikirkan Soal Sanksi dan Bata...</td>\n",
       "      <td>Senin, 29 Oktober 2018 20:18 WIB</td>\n",
       "      <td>Laporan Wartawan Tribunnews.com, Seno Tri Suli...</td>\n",
       "      <td>http://www.tribunnews.com/bisnis/2018/10/29/pe...</td>\n",
       "      <td>Laporan Wartawan Tribunnews.com, Seno Tri Suli...</td>\n",
       "      <td>jokowi</td>\n",
       "      <td>joko-widodo</td>\n",
       "      <td>Pesawat Lion Air Jatuh, Lion Air, Joko Widodo,...</td>\n",
       "      <td>Choirul Arifin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jokowi: Kita Lakukan Upaya Terbaik Menemukan d...</td>\n",
       "      <td>Senin, 29 Oktober 2018 15:12 WIB</td>\n",
       "      <td>TRIBUNNEWS.COM, JAKARTA</td>\n",
       "      <td>http://www.tribunnews.com/nasional/2018/10/29/...</td>\n",
       "      <td>TRIBUNNEWS.COM, JAKARTA - Presiden Joko Widodo...</td>\n",
       "      <td>jokowi</td>\n",
       "      <td>joko-widodo</td>\n",
       "      <td>Pesawat Lion Air Jatuh, Joko Widodo, Basarnas,...</td>\n",
       "      <td>Johnson Simanjuntak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TKN Jokowi-Ma'ruf: Salam Satu Jempol Kobarkan ...</td>\n",
       "      <td>Senin, 29 Oktober 2018 08:31 WIB</td>\n",
       "      <td>Laporan Wartawan Tribunnews.com, Dennis Destry...</td>\n",
       "      <td>http://www.tribunnews.com/nasional/2018/10/29/...</td>\n",
       "      <td>Laporan Wartawan Tribunnews.com, Dennis Destry...</td>\n",
       "      <td>jokowi</td>\n",
       "      <td>joko-widodo</td>\n",
       "      <td>Pilpres 2019, Hasto Kristiyanto, Joko Widodo, ...</td>\n",
       "      <td>Hendra Gunawan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Silaturahmi dengan Pimpinan Ponpes se-Malang R...   \n",
       "1  Indonesia Memiliki Program Pengurangan Polusi ...   \n",
       "2  Pemerintah Belum Pikirkan Soal Sanksi dan Bata...   \n",
       "3  Jokowi: Kita Lakukan Upaya Terbaik Menemukan d...   \n",
       "4  TKN Jokowi-Ma'ruf: Salam Satu Jempol Kobarkan ...   \n",
       "\n",
       "                              dates  \\\n",
       "0  Senin, 29 Oktober 2018 21:37 WIB   \n",
       "1  Senin, 29 Oktober 2018 20:21 WIB   \n",
       "2  Senin, 29 Oktober 2018 20:18 WIB   \n",
       "3  Senin, 29 Oktober 2018 15:12 WIB   \n",
       "4  Senin, 29 Oktober 2018 08:31 WIB   \n",
       "\n",
       "                                            location  \\\n",
       "0                           TRIBUNNEWS.COM, MALANG -   \n",
       "1                             TRIBUNNEWS.COM, BADUNG   \n",
       "2  Laporan Wartawan Tribunnews.com, Seno Tri Suli...   \n",
       "3                            TRIBUNNEWS.COM, JAKARTA   \n",
       "4  Laporan Wartawan Tribunnews.com, Dennis Destry...   \n",
       "\n",
       "                                                link  \\\n",
       "0  http://www.tribunnews.com/regional/2018/10/29/...   \n",
       "1  http://www.tribunnews.com/nasional/2018/10/29/...   \n",
       "2  http://www.tribunnews.com/bisnis/2018/10/29/pe...   \n",
       "3  http://www.tribunnews.com/nasional/2018/10/29/...   \n",
       "4  http://www.tribunnews.com/nasional/2018/10/29/...   \n",
       "\n",
       "                                           paragraph   label          tag  \\\n",
       "0  TRIBUNNEWS.COM, MALANG - Calon wakil presiden ...  jokowi  joko-widodo   \n",
       "1  TRIBUNNEWS.COM, BADUNG - Presiden Joko Widodo ...  jokowi  joko-widodo   \n",
       "2  Laporan Wartawan Tribunnews.com, Seno Tri Suli...  jokowi  joko-widodo   \n",
       "3  TRIBUNNEWS.COM, JAKARTA - Presiden Joko Widodo...  jokowi  joko-widodo   \n",
       "4  Laporan Wartawan Tribunnews.com, Dennis Destry...  jokowi  joko-widodo   \n",
       "\n",
       "                                            keywords               editor  \n",
       "0  Pilpres 2019, Maruf Amin, Malang, cawapres, Jo...            Sugiyarto  \n",
       "1  Joko Widodo, Our Ocean Conference 2018, Nusa D...      Toni Bramantoro  \n",
       "2  Pesawat Lion Air Jatuh, Lion Air, Joko Widodo,...       Choirul Arifin  \n",
       "3  Pesawat Lion Air Jatuh, Joko Widodo, Basarnas,...  Johnson Simanjuntak  \n",
       "4  Pilpres 2019, Hasto Kristiyanto, Joko Widodo, ...       Hendra Gunawan  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['joko-widodo', 'prabowo-subianto'] #list tags yang mau di crawl\n",
    "nama_label = ['jokowi', 'prabowo'] #nama label dari setiap tag\n",
    "tanggal_mulai = '2018/08/01' #tanggal awal pencarian\n",
    "tanggal_akhir = '2018/10/29' #tanggal akhir pencarian\n",
    "\n",
    "titles, links, label, tag = outside_crawl(tags, nama_label, tanggal_mulai, tanggal_akhir) #crawl bagian luar berita (mengakses index)\n",
    "times, locations, keywords, paragraphs, editors = inside_crawl(links) #crawl isi berita (sudah mengakses link berita)\n",
    "\n",
    "tribun = {'headline' :titles, 'dates' :times, 'location' :locations, 'link' :links,\n",
    "          'paragraph' :paragraphs, 'label' :label, 'tag' :tag, 'keywords' :keywords, 'editor' :editors} #Data Framing step 1\n",
    "\n",
    "df_tribun = pd.DataFrame(tribun, columns = tribun.keys()) #Data Framing step 2\n",
    "df_tribun.to_csv('output.csv')\n",
    "df_tribun.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
